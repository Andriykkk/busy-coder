# TODO: Implement Model Evaluation

This file outlines the steps to implement a comprehensive evaluation for the model.

## 1. Environment Setup

- [ ] Create a conda environment from the `environment.yml` file in `code-act`.
- [ ] Install all necessary dependencies from `requirements.txt` files in `code-act` and its submodules.
- [ ] Set up the `OPENAI_API_KEY` as an environment variable.

## 2. Data Setup

- [ ] Run the `setup_data.sh` script in `code-act/scripts/eval` to download and prepare all evaluation datasets.

## 3. Model Serving

- [ ] Implement a script to serve the model using an OpenAI-compatible API. This can be based on vLLM or another serving framework. The script should be configurable to load different model checkpoints.

## 4. Evaluation Benchmarks

Implement scripts to run the following benchmarks. Each script should:
- Take the model endpoint as an argument.
- Run the evaluation for the specific benchmark.
- Save the results in a structured format (e.g., JSON) in a designated output directory.

### 4.1. M³ToolEval

- [ ] Implement a script to run the M³ToolEval benchmark.
- [ ] The script should be able to run the evaluation for different action modes (text, json, code).
- [ ] The script should be able to filter tasks using regex.

### 4.2. API-Bank

- [ ] Implement a script to run the API-Bank benchmark.

### 4.3. MINT-Bench

- [ ] Implement a script to run the MINT-Bench benchmark.

### 4.4. MiniWob++

- [ ] Implement a script to run the MiniWob++ benchmark.

### 4.5. ScienceWorld

- [ ] Implement a script to run the ScienceWorld benchmark.

### 4.6. GSM-8K

- [ ] Implement a script to run the GSM-8K benchmark.

### 4.7. HumanEval

- [ ] Implement a script to run the HumanEval benchmark.

### 4.8. MMLU

- [ ] Implement a script to run the MMLU benchmark.

### 4.9. MT-Bench

- [ ] Implement a script to run the MT-Bench benchmark.

## 5. Aggregation

- [ ] Implement a script to aggregate the results from all benchmarks into a single report. This can be based on `aggregate_eval.py` in `code-act/scripts/eval`.

## 6. Main Evaluation Script

- [ ] Create a main script that orchestrates the entire evaluation process:
    - Takes a model checkpoint as input.
    - Starts the model serving.
    - Runs all the evaluation benchmarks in parallel.
    - Stops the model serving.
    - Aggregates the results.
- [ ] This script can be inspired by `run_all.sh` in `code-act/scripts/eval`.
